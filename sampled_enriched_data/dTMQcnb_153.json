{
  "id": "dTMQcnb_153",
  "conversations": [
    {
      "from": "gpt",
      "value": "Sure, here's an example of how to use the Hugging Face Trainer to train your model using the provided `train.csv` and `val.csv` files:\n```python\nimport os\nimport random\nimport numpy as np\nimport torch\nfrom transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# Set random seeds for reproducibility\nrandom.seed(1234)\nnp.random.seed(1234)\ntorch.manual_seed(1234)\ntorch.cuda.manual_seed_all(1234)\n\n# Set the device to run on\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the tokenizer and model\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=8)\n\n# Set up the data loader\ntrain_loader = DataLoader.from_dataset(\n    torch.utils.data.TensorDataset(torch.load('./data/train_input_ids.pt'),\n                                   torch.load('./data/train_attention_mask.pt'),\n                                   torch.load('./data/train_labels.pt')),\n    batch_size=16,\n    shuffle=True,\n)\nval_loader = DataLoader.from_dataset(\n    torch.utils.data.TensorDataset(torch.load('./data/val_input_ids.pt'),\n                                   torch.load('./data/val_attention_mask.pt'),\n                                   torch.load('./data/val_labels.pt')),\n    batch_size=16,\n)\n\n# Set up the training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=10,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    gradient_accumulation_steps=1,\n    evaluation_strategy='epoch',\n    learning_rate=2e-5,\n    lr_scheduler_type='linear',\n    warmup_steps=0,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model='eval_accuracy',\n    greater_is_better=True,\n    disable_tqdm=False,\n)\n\n# Define the compute_metrics function\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n    acc = accuracy_score(labels, predictions)\n    return {\n        'accuracy': acc,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n    }\n\n# Set up the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_loader,\n    eval_dataset=val_loader,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n)\n\n# Train the model\ntrainer.train()\n```\nIn this example, we're using the `BertTokenizerFast` and `BertForSequenceClassification` classes from the Transformers library, as well as the `Trainer` and `TrainingArguments` classes from the Hugging Face library to set up and run the training process. We're also defining a `compute_metrics` function to compute additional evaluation metrics beyond the default accuracy.",
      "language": "en",
      "sentiment": {
        "neg": 0.011,
        "neu": 0.94,
        "pos": 0.05,
        "compound": 0.8271
      },
      "core_lexical": {
        "self_ref_freq": 2,
        "second_person_freq": 3,
        "token_count": 407,
        "self_ref_ratio": 0.004914004914004914,
        "second_person_ratio": 0.007371007371007371
      },
      "lexical_features": {
        "adj_adv_freq": 27,
        "adj_adv_ratio": 0.06633906633906633,
        "self_ref_freq": 2,
        "self_ref_ratio": 0.004914004914004914,
        "second_person_freq": 3,
        "second_person_ratio": 0.007371007371007371,
        "token_count": 407
      },
      "advanced_features": {
        "structural_features": {
          "avg_sentence_length": 203.5,
          "clause_complexity": 0.5,
          "vocabulary_richness": 0.5504587155963303
        },
        "interaction_features": {
          "question_types": {
            "what": 0,
            "how": 1,
            "why": 0,
            "can_could": 0,
            "general": 0
          },
          "discourse_markers": {
            "elaboration": 0,
            "contrast": 0,
            "causality": 0,
            "hedging": 0
          }
        },
        "linguistic_features": {
          "verb_tense_dist": {
            "VBZ": 1,
            "VB": 7,
            "VBG": 3,
            "VBP": 9,
            "VBN": 2,
            "VBD": 1
          },
          "named_entities": {
            "ORGANIZATION": 8,
            "PERSON": 1
          }
        }
      }
    },
    {
      "from": "human",
      "value": "Above code I dont have train\\_loader = DataLoader.from\\_dataset(\n torch.utils.data.TensorDataset(torch.load('./data/train\\_input\\_ids.pt'),\n torch.load('./data/train\\_attention\\_mask.pt'),\n torch.load('./data/train\\_labels.pt')),",
      "language": "en",
      "sentiment": {
        "neg": 0.0,
        "neu": 1.0,
        "pos": 0.0,
        "compound": 0.0
      },
      "core_lexical": {
        "self_ref_freq": 1,
        "second_person_freq": 0,
        "token_count": 30,
        "self_ref_ratio": 0.03333333333333333,
        "second_person_ratio": 0.0
      },
      "lexical_features": {
        "adj_adv_freq": 1,
        "adj_adv_ratio": 0.03333333333333333,
        "self_ref_freq": 1,
        "self_ref_ratio": 0.03333333333333333,
        "second_person_freq": 0,
        "second_person_ratio": 0.0,
        "token_count": 30
      },
      "advanced_features": {
        "structural_features": {
          "avg_sentence_length": 30.0,
          "clause_complexity": 0.0,
          "vocabulary_richness": 1.0
        },
        "interaction_features": {
          "question_types": {
            "what": 0,
            "how": 0,
            "why": 0,
            "can_could": 0,
            "general": 0
          },
          "discourse_markers": {
            "elaboration": 0,
            "contrast": 0,
            "causality": 0,
            "hedging": 0
          }
        },
        "linguistic_features": {
          "verb_tense_dist": {
            "VBP": 2,
            "VBN": 1
          },
          "named_entities": {}
        }
      },
      "expressive_label": 0,
      "anthropomorphic_label": 0
    },
    {
      "from": "gpt",
      "value": "You are right, apologies for the mistake in the previous code. Here is the updated code using the `DataLoader.from_dataset` method:\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom transformers import AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\n\ntrain_input_ids = torch.load('./data/train_input_ids.pt')\ntrain_attention_mask = torch.load('./data/train_attention_mask.pt')\ntrain_labels = torch.load('./data/train_labels.pt')\n\ntrain_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\ntrain_loader = DataLoader.from_dataset(train_dataset, batch_size=8, shuffle=True)\n\nval_input_ids = torch.load('./data/val_input_ids.pt')\nval_attention_mask = torch.load('./data/val_attention_mask.pt')\nval_labels = torch.load('./data/val_labels.pt')\n\nval_dataset = TensorDataset(val_input_ids, val_attention_mask, val_labels)\nval_loader = DataLoader.from_dataset(val_dataset, batch_size=8, shuffle=True)\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=10,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=0,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model='accuracy',\n    evaluation_strategy='epoch',\n)\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=8)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n                                'attention_mask': torch.stack([f[1] for f in data]),\n                                'labels': torch.stack([f[2] for f in data])},\n    compute_metrics=lambda pred: {'accuracy': (pred.predictions.argmax(-1) == pred.label_ids).float().mean()}\n)\n\ntrainer.train()\n```",
      "language": "en",
      "sentiment": {
        "neg": 0.017,
        "neu": 0.983,
        "pos": 0.0,
        "compound": -0.34
      },
      "core_lexical": {
        "self_ref_freq": 0,
        "second_person_freq": 1,
        "token_count": 278,
        "self_ref_ratio": 0.0,
        "second_person_ratio": 0.0035971223021582736
      },
      "lexical_features": {
        "adj_adv_freq": 22,
        "adj_adv_ratio": 0.07913669064748201,
        "self_ref_freq": 0,
        "self_ref_ratio": 0.0,
        "second_person_freq": 1,
        "second_person_ratio": 0.0035971223021582736,
        "token_count": 278
      },
      "advanced_features": {
        "structural_features": {
          "avg_sentence_length": 139.0,
          "clause_complexity": 0.0,
          "vocabulary_richness": 0.574468085106383
        },
        "interaction_features": {
          "question_types": {
            "what": 0,
            "how": 0,
            "why": 0,
            "can_could": 0,
            "general": 0
          },
          "discourse_markers": {
            "elaboration": 0,
            "contrast": 0,
            "causality": 0,
            "hedging": 0
          }
        },
        "linguistic_features": {
          "verb_tense_dist": {
            "VBP": 9,
            "VBZ": 2,
            "VBG": 1,
            "VB": 1,
            "VBD": 4
          },
          "named_entities": {
            "ORGANIZATION": 6,
            "PERSON": 1
          }
        }
      }
    },
    {
      "from": "human",
      "value": "No you create wrong answear in my data folder just have train.csv and val.csv",
      "language": "en",
      "sentiment": {
        "neg": 0.393,
        "neu": 0.607,
        "pos": 0.0,
        "compound": -0.7281
      },
      "core_lexical": {
        "self_ref_freq": 1,
        "second_person_freq": 1,
        "token_count": 14,
        "self_ref_ratio": 0.07142857142857142,
        "second_person_ratio": 0.07142857142857142
      },
      "lexical_features": {
        "adj_adv_freq": 2,
        "adj_adv_ratio": 0.14285714285714285,
        "self_ref_freq": 1,
        "self_ref_ratio": 0.07142857142857142,
        "second_person_freq": 1,
        "second_person_ratio": 0.07142857142857142,
        "token_count": 14
      },
      "advanced_features": {
        "structural_features": {
          "avg_sentence_length": 14.0,
          "clause_complexity": 0.0,
          "vocabulary_richness": 1.0
        },
        "interaction_features": {
          "question_types": {
            "what": 0,
            "how": 0,
            "why": 0,
            "can_could": 0,
            "general": 0
          },
          "discourse_markers": {
            "elaboration": 0,
            "contrast": 0,
            "causality": 0,
            "hedging": 0
          }
        },
        "linguistic_features": {
          "verb_tense_dist": {
            "VBP": 1,
            "VB": 1
          },
          "named_entities": {}
        }
      },
      "expressive_label": 0,
      "anthropomorphic_label": 0
    },
    {
      "from": "gpt",
      "value": "I see, in that case, you can modify the code to read the CSV files instead of the pre-processed tensors. Here's how you can modify the code to use CSV files:\n```python\nimport torch\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom dataloader import get_data_loaders\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\ntrain_csv_path = './data/train.csv'\nval_csv_path = './data/val.csv'\nmax_len = 512\nbatch_size = 8\nnum_workers = 4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntrain_loader, val_loader = get_data_loaders(train_csv_path, val_csv_path, tokenizer, max_len, batch_size, num_workers, device)\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=8)\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=10,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=2e-5,\n    warmup_steps=0,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_loader.dataset,\n    eval_dataset=val_loader.dataset,\n    data_collator=lambda data: {'input_ids': torch.stack([item[0] for item in data]),\n                                'attention_mask': torch.stack([item[1] for item in data]),\n                                'labels': torch.stack([item[2] for item in data])},\n)\n\ntrainer.train()\n```\nHere, the `train_dataset` and `eval_dataset` are set to the `dataset` attribute of the `train_loader` and `val_loader` respectively. The `data_collator` is a function that groups the data into batches and applies any necessary preprocessing to the data. In this case, the `data_collator` stacks the tensors for each batch.\n\nAlso note that since we are using the `Trainer` class from Hugging Face, we no longer need to manually define the training loop. Instead, we simply call the `train()` method on the `trainer` object to start the training process.",
      "language": "en",
      "sentiment": {
        "neg": 0.01,
        "neu": 0.967,
        "pos": 0.023,
        "compound": 0.4588
      },
      "core_lexical": {
        "self_ref_freq": 4,
        "second_person_freq": 5,
        "token_count": 342,
        "self_ref_ratio": 0.011695906432748537,
        "second_person_ratio": 0.014619883040935672
      },
      "lexical_features": {
        "adj_adv_freq": 24,
        "adj_adv_ratio": 0.07017543859649122,
        "self_ref_freq": 4,
        "self_ref_ratio": 0.011695906432748537,
        "second_person_freq": 5,
        "second_person_ratio": 0.014619883040935672,
        "token_count": 342
      },
      "advanced_features": {
        "structural_features": {
          "avg_sentence_length": 57.0,
          "clause_complexity": 0.16666666666666666,
          "vocabulary_richness": 0.6597938144329897
        },
        "interaction_features": {
          "question_types": {
            "what": 0,
            "how": 1,
            "why": 0,
            "can_could": 2,
            "general": 0
          },
          "discourse_markers": {
            "elaboration": 0,
            "contrast": 0,
            "causality": 0,
            "hedging": 0
          }
        },
        "linguistic_features": {
          "verb_tense_dist": {
            "VBP": 9,
            "VB": 12,
            "VBZ": 3,
            "VBN": 2,
            "VBD": 4,
            "VBG": 2
          },
          "named_entities": {
            "ORGANIZATION": 7,
            "PERSON": 2
          }
        }
      }
    }
  ],
  "overall_sentiment": {
    "compound": 0.04355999999999999,
    "pos": 0.014600000000000002,
    "neu": 0.8994,
    "neg": 0.08620000000000001
  },
  "overall_lexical": {
    "adj_adv_freq": 76,
    "adj_adv_ratio": 0.07096171802054155,
    "self_ref_freq": 8,
    "self_ref_ratio": 0.007469654528478058,
    "second_person_freq": 10,
    "second_person_ratio": 0.009337068160597572,
    "total_tokens": 1071
  },
  "overall_advanced": {
    "structural_features": {
      "avg_sentence_length": 88.7,
      "clause_complexity": 0.13333333333333333,
      "vocabulary_richness": 0.7569441230271405
    },
    "interaction_features": {
      "question_types": {
        "what": 0,
        "how": 2,
        "why": 0,
        "can_could": 2,
        "general": 0
      },
      "discourse_markers": {
        "elaboration": 0,
        "contrast": 0,
        "causality": 0,
        "hedging": 0
      }
    },
    "linguistic_features": {
      "verb_tense_dist": {
        "VBZ": 6,
        "VB": 21,
        "VBG": 6,
        "VBP": 30,
        "VBN": 5,
        "VBD": 9
      },
      "named_entities": {
        "ORGANIZATION": 21,
        "PERSON": 4
      }
    }
  },
  "conversation_metadata": {
    "language_distribution": {
      "en": 5
    },
    "turn_count": 5,
    "total_tokens": 1071
  }
}